{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Translator_with_transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NgDNRlN1NFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import re\n",
        "import time\n",
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaeFoF3L3w1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbwAexb54uf1",
        "colab_type": "code",
        "outputId": "cef31632-1a06-42bf-e4b5-a76a48cedf12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-RwWTE94ucm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('./europarl-v7.fr-en.en',\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "          europarl_en = f.read()\n",
        "\n",
        "with open('./europarl-v7.fr-en.fr',\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "          europarl_fr = f.read()\n",
        "\n",
        "with open('./nonbreaking_prefix.en',\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "          nonbreaking_prefix_en = f.read()\n",
        "\n",
        "with open('./nonbreaking_prefix.fr',\n",
        "          mode='r',\n",
        "          encoding='utf-8') as f:\n",
        "          nonbreaking_prefix_fr = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF_0WREIkfRd",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIYYaNGxO_0T",
        "colab_type": "code",
        "outputId": "9ca6a38f-b3b1-41c6-f129-2d91d2775502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(europarl_fr),len(europarl_en)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(335706962, 301210536)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOA6ewZx4uVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nonbreaking_prefix_en = nonbreaking_prefix_en.split('\\n')\n",
        "nonbreaking_prefix_en = [' ' + pref + '.' for pref in nonbreaking_prefix_en]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-qO_6SP4uRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nonbreaking_prefix_fr = nonbreaking_prefix_fr.split('\\n')\n",
        "nonbreaking_prefix_fr = [' ' + pref + '.' for pref in nonbreaking_prefix_fr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p9IUwJJ4uJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiMfhKIv4uCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_en = europarl_en\n",
        "for prefix in nonbreaking_prefix_en:\n",
        "  corpus_en = corpus_en.replace(prefix, prefix +'###')\n",
        "corpus_en = re.sub(r'\\.(?=[0-9]|[a-z]|[A-Z])','.###',corpus_en)\n",
        "corpus_en = re.sub(r'.\\#\\#\\#',' ',corpus_en)\n",
        "corpus_en = re.sub(r'  +',' ',corpus_en)\n",
        "corpus_en = corpus_en.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAoZa90q4t6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_fr = europarl_fr\n",
        "for prefix in nonbreaking_prefix_fr:\n",
        "  corpus_fr = corpus_fr.replace(prefix, prefix +'###')\n",
        "corpus_fr = re.sub(r'\\.(?=[0-9]|[a-z]|[A-Z])','.###',corpus_fr)\n",
        "corpus_fr = re.sub(r'.\\#\\#\\#',' ',corpus_fr)\n",
        "corpus_fr = re.sub(r'  +',' ',corpus_fr)\n",
        "corpus_fr = corpus_fr.split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTXLo7ztnPsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(corpus_en),len(corpus_fr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tizu76PwHH_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_en,target_vocab_size=2**13\n",
        ")\n",
        "tokenizer_fr = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    corpus_fr,target_vocab_size=2**13\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egI7Bu7wYqpW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2 \n",
        "VOCAB_SIZE_FR = tokenizer_fr.vocab_size + 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJKHlIxxZm9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = [[VOCAB_SIZE_EN - 2] +tokenizer_en.encode(sentence)  + [VOCAB_SIZE_EN-1] for sentence in corpus_en]\n",
        "outputs = [[VOCAB_SIZE_FR - 2] +tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR-1] for sentence in corpus_fr]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rxn4ndak6eM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(inputs),len(outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nxD-z1tFveR",
        "colab_type": "text"
      },
      "source": [
        " Without reversed won't work because each time you remove an element, the indices of the list are shifted by one. So you need to start by the end of the list so that you are always referring to the good index.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70Pfh0XqZm6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 20\n",
        "idx_to_remove = [count for count,sentence in enumerate(inputs) if len(sentence) > MAX_LEN]\n",
        "print('input length: ',len(inputs))\n",
        "print('output length: ',len(outputs))\n",
        "for id in reversed(idx_to_remove):\n",
        "  #if id < len(inputs) and id < len(outputs):\n",
        "    del inputs[id]\n",
        "    del outputs[id]\n",
        "\n",
        "idx_to_remove = [count for count,sentence in enumerate(outputs) if len(sentence) > MAX_LEN]\n",
        "for id in reversed(idx_to_remove):\n",
        "  #if id < len(inputs) and id < len(outputs):\n",
        "    del inputs[id]\n",
        "    del outputs[id]\n",
        "print('input length: ',len(inputs))\n",
        "print('output length: ',len(outputs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqQtuE7qZmzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for sent in inputs:\n",
        "  print(len(sent))\n",
        "  print(sent)\n",
        "  print(tokenizer_en.decode(sent[1:len(sent)-1]))\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cXXECkNj5Ti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hZKxZlVd34P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,                                                      \n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=20)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
        "                                                        value=0,\n",
        "                                                        padding='post',\n",
        "                                                        maxlen=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JXfgAkViKAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(inputs[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNo4CIuKlc6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDag7Holl8oO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((inputs,outputs))\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pReVJBvQmo-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "\n",
        "  def get_angles(self,pos,i,d_model):\n",
        "    angles = 1 / np.power(10000.,(2*(i//2))) / (np.float32(d_model))\n",
        "    return pos * angles\n",
        "\n",
        "  def call(self,inputs):\n",
        "    seq_length = inputs.shape.as_list()[-2]\n",
        "    d_model = inputs.shape.as_list()[-1]\n",
        "\n",
        "    angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                             np.arange(d_model)[np.newaxis, :],\n",
        "                             d_model)\n",
        "    angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "    angles[:, 1::2] = np.cos(angles[:, 1:2])\n",
        "    pos_encoding = angles[np.newaxis, ...]\n",
        "\n",
        "    return inputs + tf.cast(pos_encoding, tf.float32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RtU1Lo85s1f",
        "colab_type": "text"
      },
      "source": [
        "## Scaled Dot-Product Attention\n",
        "\n",
        "Attention(Q, K, V) = softmax((QK.T) / sqrt(dk))*V"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKZkH8LytUTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product(queries, keys, values, mask):\n",
        "  product = tf.matmul(queries, keys, transpose_b=True)\n",
        "  \n",
        "  keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32) # want it to be a float that why we do cast and take the last composant\n",
        "  scaled_product = product / tf.math.sqrt(keys_dim)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_product += (mask * -1e9)\n",
        "\n",
        "  attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
        "  return attention\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeY5aQld7mpR",
        "colab_type": "text"
      },
      "source": [
        "## Multi-head attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAvnL1aOtVzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "  def __init__(self, nb_proj):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.nb_proj = nb_proj\n",
        "\n",
        "  def build(self, input_shape) : #Initialization like init but will be called when we call the object for the first time\n",
        "    #complete the initialization phase but will have to much more information because it will have input when being called  \n",
        "    self.d_model = input_shape[-1]\n",
        "    assert self.d_model % self.nb_proj == 0\n",
        "\n",
        "    self.d_proj = self.d_model // self.nb_proj\n",
        "    \n",
        "    self.query_lin = layers.Dense(units=self.d_model)\n",
        "    self.key_lin = layers.Dense(units=self.d_model)\n",
        "    self.value_lin = layers.Dense(units=self.d_model)\n",
        "\n",
        "    self.final_linear = layers.Dense(units=self.d_model)\n",
        "  \n",
        "  def split_proj(self, inputs, batch_size):#inputs (batch, seq_length, d_model):\n",
        "    shape = (batch_size,\n",
        "             -1,\n",
        "             self.nb_proj,\n",
        "             self.d_proj)\n",
        "    splitted_inputs = tf.reshape(inputs, shape=shape) #(batch_size, seq_length, nb_proj, d_proj)\n",
        "    return tf.transpose(splitted_inputs, perm=[0, 2, 1, 3])#(batch_size, nb_proj, seq_length, d_proj)\n",
        "\n",
        "  def call(self, queries, keys, values, mask):\n",
        "    batch_size = tf.shape(queries)[0]\n",
        "\n",
        "    queries = self.query_lin(queries)\n",
        "    keys = self.key_lin(keys)\n",
        "    values =  self.value_lin(values)\n",
        "\n",
        "    queries = self.split_proj(queries, batch_size)\n",
        "    keys = self.split_proj(keys, batch_size)\n",
        "    values = self.split_proj(values, batch_size)#attend to more information and to be able to get more relations between the elements of a sequence\n",
        "\n",
        "    attention = scaled_dot_product(queries, keys, values, mask)\n",
        "\n",
        "    attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "    concat_attention = tf.reshape(attention, \n",
        "                                  shape= (batch_size, -1, self.d_model))\n",
        "    \n",
        "    outputs = self.final_linear(concat_attention)\n",
        "    return outputs\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waylz6vci74G",
        "colab_type": "text"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqjP9n7StVsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "  def __init__(self, FNN_units, nb_proj, dropout):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.FNN_units = FNN_units\n",
        "    self.nb_proj = nb_proj\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "\n",
        "    self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "    self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dense_1 = layers.Dense(units=self.FNN_units, activation='relu')\n",
        "    self.dense_2 = layers.Dense(units=self.d_model)\n",
        "    self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "    self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "\n",
        "  def call(self, inputs, mask, training):\n",
        "    attention = self.multi_head_attention(inputs,\n",
        "                                          inputs,\n",
        "                                          inputs,\n",
        "                                          mask)\n",
        "    #print(attention.shape, inputs.shape)\n",
        "    attention = self.dropout_1(attention, training=training)\n",
        "    attention = self.norm_1(attention + inputs)\n",
        "\n",
        "    outputs = self.dense_1(attention)\n",
        "    outputs = self.dense_2(outputs)\n",
        "    outputs = self.dropout_2(outputs)\n",
        "    outputs = self.norm_2(outputs + attention)\n",
        "\n",
        "    return outputs\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4KfIA_8tVmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "  def __init__(self,\n",
        "               nb_layers,\n",
        "               FNN_units,\n",
        "               nb_proj,\n",
        "               dropout,\n",
        "               vocab_size,\n",
        "               d_model,\n",
        "               name='encoder'):\n",
        "    super(Encoder, self).__init__(name = name)\n",
        "    self.nb_layers = nb_layers\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding()\n",
        "    self.dropout = layers.Dropout(rate=dropout)\n",
        "    self.enc_layers = [EncoderLayer(FNN_units,\n",
        "                                  nb_proj,\n",
        "                                  dropout)\n",
        "                    for _ in range(self.nb_layers)]\n",
        "\n",
        "  def call(self, inputs, mask, training):\n",
        "    outputs = self.embedding(inputs)\n",
        "    outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    outputs = self.pos_encoding(outputs)\n",
        "    outputs = self.dropout(outputs, training)\n",
        "\n",
        "    for i in range(self.nb_layers):\n",
        "     outputs = self.enc_layers[i](outputs, mask, training)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beN9GCUptVhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O6PZMpMtMxr",
        "colab_type": "text"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eth8Je_HtVbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "  def __init__(self, FNN_units, nb_proj, dropout):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "    self.FNN_units = FNN_units\n",
        "    self.nb_proj = nb_proj\n",
        "    self.dropout = dropout\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.d_model = input_shape[-1]\n",
        "\n",
        "    #self multi head attention\n",
        "    self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "    self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    # Multi head attention combined with encoder output\n",
        "    self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "    self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    #Feed forward\n",
        "    self.dense_1 = layers.Dense(units=self.FNN_units, activation='relu')\n",
        "    self.dense_2 = layers.Dense(units=self.d_model)\n",
        "    self.dropout_3 = layers.Dropout(rate=self.dropout)\n",
        "    self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "\n",
        "  def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "    attention = self.multi_head_attention_1(inputs,\n",
        "                                          inputs,\n",
        "                                          inputs,\n",
        "                                          mask_1)\n",
        "    attention = self.dropout_1(attention, training)\n",
        "    attention = self.norm_1(attention + inputs)\n",
        "\n",
        "    attention_2 = self.multi_head_attention_2(attention,\n",
        "                                          enc_outputs,\n",
        "                                          enc_outputs,\n",
        "                                          mask_2)\n",
        "\n",
        "    attention_2 = self.dropout_2(attention_2, training)\n",
        "    attention_2 = self.norm_2(attention_2 + attention)\n",
        "\n",
        "   \n",
        "    outputs = self.dense_1(attention_2)\n",
        "    outputs = self.dense_2(outputs)\n",
        "    outputs = self.dropout_3(outputs, training)\n",
        "    outputs = self.norm_3(outputs + attention_2)\n",
        "\n",
        "\n",
        "    return outputs\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPBhJWUltVTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "  def __init__(self,\n",
        "               nb_layers,\n",
        "               FNN_units,\n",
        "               nb_proj,\n",
        "               dropout,\n",
        "               vocab_size,\n",
        "               d_model,\n",
        "               name='decoder'):\n",
        "    super(Decoder, self).__init__(name = name)\n",
        "    self.nb_layers = nb_layers\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding()\n",
        "    self.dropout = layers.Dropout(rate=dropout)\n",
        "    self.dec_layers = [DecoderLayer(FNN_units,\n",
        "                                  nb_proj,\n",
        "                                  dropout)\n",
        "                    for _ in range(self.nb_layers)]\n",
        "\n",
        "   \n",
        "\n",
        "  def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "    outputs = self.embedding(inputs)\n",
        "    outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) #normalization\n",
        "    outputs = self.pos_encoding(outputs)\n",
        "    outputs = self.dropout(outputs, training)\n",
        "\n",
        "    for i in range(self.nb_layers):\n",
        "      outputs = self.dec_layers[i](outputs,\n",
        "                                   enc_outputs,\n",
        "                                   mask_1,\n",
        "                                   mask_2,\n",
        "                                   training)\n",
        "\n",
        "    return outputs\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oUncNBWy_gI",
        "colab_type": "text"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM9nvkXmtVPO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "               vocab_size_enc,\n",
        "               vocab_size_dec,\n",
        "               d_model,\n",
        "               nb_layers,\n",
        "               FNN_units,\n",
        "               nb_proj,\n",
        "               dropout,\n",
        "               name = 'transformer'\n",
        "               ):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.encoder = Encoder(nb_layers,\n",
        "                           FNN_units,\n",
        "                           nb_proj,\n",
        "                           dropout,\n",
        "                           vocab_size_enc,\n",
        "                           d_model)\n",
        "    self.decoder = Decoder(nb_layers,\n",
        "                           FNN_units,\n",
        "                           nb_proj,\n",
        "                           dropout,\n",
        "                           vocab_size_dec,\n",
        "                           d_model)\n",
        "    self.last_linear = layers.Dense(units = vocab_size_dec)\n",
        "\n",
        "  def create_padding_mask(self, seq): # seq: (batch_size, seq_length)\n",
        "    mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    return mask[:, None, None, :]\n",
        "\n",
        "  def create_look_ahead_mask(self, seq):\n",
        "    seq_len = tf.shape(seq)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    return look_ahead_mask\n",
        "\n",
        "\n",
        "  def call(self, enc_inputs, dec_inputs, training):\n",
        "    enc_mask = self.create_padding_mask(enc_inputs)\n",
        "    dec_mask_1 = tf.maximum(\n",
        "        self.create_padding_mask(dec_inputs),\n",
        "        self.create_look_ahead_mask(dec_inputs)\n",
        "    )\n",
        "    dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "\n",
        "    enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "    dec_outputs = self.decoder(dec_inputs,\n",
        "                              enc_outputs,\n",
        "                              dec_mask_1,\n",
        "                              dec_mask_2,\n",
        "                              training)\n",
        "    \n",
        "    outputs = self.last_linear(dec_outputs)\n",
        "    return outputs\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq8brMTa8GEU",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLXdIUu5tVJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hyper-parameters\n",
        "D_MODEL = 128 # 512\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT_RATE = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_FR,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FNN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout=DROPOUT_RATE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaQgkA_7tVHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction='none')\n",
        "\n",
        "def loss_function(target, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "  loss_ = loss_object(target, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1wdeoP4tVDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = tf.cast(d_model, tf.float32)\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps**-1.5)\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                    beta_1=0.9,\n",
        "                                    beta_2=0.98,\n",
        "                                    epsilon=1e-9)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On4Y43GytVAN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = './drive/ My DRive/projects/transformer/ckpt'\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restart(ckpt_manager.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY7UZqV_tU75",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 2\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f'Start of epoch{epoch+1}')\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "    dec_inputs = targets[:, :-1]\n",
        "    dec_outputs_real = targets[:, 1:]\n",
        "    with tf.GradientTape() as taps:\n",
        "      predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "      loss = loss_function(dec_outputs_real, predictions)\n",
        "    \n",
        "    gradients = taps.gradient(loss, transformer.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(dec_outputs_real, predictions)\n",
        "    if batch % 50 == 0:\n",
        "          print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f%}\".format(\n",
        "              epoch+1, batch, train_loss.result(), 100.*train_accuracy.result()))\n",
        "          \n",
        "  ckpt_save_path = ckpt_manager.save()\n",
        "  print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
        "                                                      ckpt_save_path))\n",
        "  print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_tVxwQBcp9P",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQSSQybPcrpL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "  inp_sentence = [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [ VOCAB_SIZE_EN-1]\n",
        "  enc_input = tf.expand_dims(inp_sentence, axis=0) #Simulate the bathc dimension\n",
        "\n",
        "  dec_output = tf.expand_dims([VOCAB_SIZE_FR-2], axis=0) #which is also the decoder input\n",
        "\n",
        "  #make several iteration of the transformer\n",
        "  for _ in range(MAX_LEN):\n",
        "    predictions = transformer(enc_input, dec_output, False)#(1, seq_len, vocab_size_fr)\n",
        "\n",
        "    prediction = predictions[:, -1:, :]\n",
        "\n",
        "    #index of the next word\n",
        "    predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "\n",
        "    if predicted_id == VOCAB_SIZE_FR-1:#Translation is done\n",
        "      return tf.squeeze(dec_output, axis=0) # get read of the batch dimension\n",
        "\n",
        "    dec_output = tf.concat([dec_ouput, predicted_id], axis=-1)\n",
        "  return tf.squeeze(dec_output, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_bTXxqWg5Te",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  output = evaluate(sentence).numpy()\n",
        "\n",
        "  predicted_sentence = tokenizer_fr.decode(\n",
        "      [i for i in output if i < VOCAB_SIZE_FR-2]\n",
        "  )\n",
        "  print(f'Input: {sentence}')\n",
        "  print(f'Predicted translation: {predicted_sentence}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I27NC2Skh12N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translate('It was so wonderful to build this application and I have learned a lot from this course and really enjoy it!!!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wlWmoL0iGXG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
